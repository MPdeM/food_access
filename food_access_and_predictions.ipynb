{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food Access Programs: Exploratory Analysis\n",
    "\n",
    "\n",
    "\n",
    "Data sources: \n",
    "\n",
    "FDA – Food Atlas - https://www.ers.usda.gov/data-products/food-environment-atlas/data-access-and-documentation-downloads/\n",
    "\n",
    "CMS – State/County Medicare Utilization Summary - https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Geographic-Variation/GV_PUF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Exploring\n",
    "The first step in the pipeline after importing is exploring the data. In this case and in the interest of time, I will do a limited version and focus the exploration to few columns. I will focus the analysis at the markers or proxis that may be associated obesity and diabetes (PCT_OBESE_ADULTS17 = Adult obesity rate, 2017 and PCT_DIABETES_ADULTS13 = Adult diabetes rate, 2009 ). Following a preliminar review of the literature, the parameters to consider are:\n",
    "\n",
    "1) parameters related to access to food (location, household resources,goverment programs..)\n",
    "\n",
    "2) parameters related to scioeconomic status (race, income level..)\n",
    "\n",
    "3) parameters related to insecurity (race, poverty level..)\n",
    "\n",
    "4) parameters related to assistance (lunch programs, ..)\n",
    "\n",
    "5) parameters related to cost (total cost, total standarized cost..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necesary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data \n",
    "\n",
    "df_food = pd.read_csv('assets/data_all_df.csv')\n",
    "df_cost = pd.read_csv('assets/data_cost_df.csv')\n",
    "df_food.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes on FIPS number\n",
    "df_all= pd.merge(df_food, df_cost, on = 'FIPS', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of values that are most interesting to food insecurity\n",
    "\n",
    "columns_general = ['FIPS', 'State', 'County', \"PCT_OBESE_ADULTS17\", \"PCT_DIABETES_ADULTS13\"]\n",
    "\n",
    "columns_access = ['LACCESS_POP10', 'LACCESS_POP15', 'PCH_LACCESS_POP_10_15',\n",
    "                  'PCT_LACCESS_POP10', 'PCT_LACCESS_POP15',\n",
    "                  'LACCESS_LOWI10', 'LACCESS_LOWI15', 'PCH_LACCESS_LOWI_10_15',\n",
    "                  'PCT_LACCESS_LOWI10', 'PCT_LACCESS_LOWI15',\n",
    "                  'LACCESS_HHNV10', 'LACCESS_HHNV15', 'PCH_LACCESS_HHNV_10_15',\n",
    "                  'PCT_LACCESS_HHNV10', 'PCT_LACCESS_HHNV15',\n",
    "                  'LACCESS_SNAP15', 'PCT_LACCESS_SNAP15',\n",
    "                  'LACCESS_CHILD10', 'LACCESS_CHILD15', 'LACCESS_CHILD_10_15',\n",
    "                  'PCT_LACCESS_CHILD10', 'PCT_LACCESS_CHILD15',\n",
    "                  'LACCESS_SENIORS10', 'LACCESS_SENIORS15', 'PCH_LACCESS_SENIORS_10_15',\n",
    "                  'PCT_LACCESS_SENIORS10', 'PCT_LACCESS_SENIORS15',\n",
    "                  'LACCESS_WHITE15', 'PCT_LACCESS_WHITE15',\n",
    "                  'LACCESS_BLACK15', 'PCT_LACCESS_BLACK15',\n",
    "                  'LACCESS_HISP15', 'PCT_LACCESS_HISP15',\n",
    "                  'LACCESS_NHASIAN15', 'PCT_LACCESS_NHASIAN15',\n",
    "#                   'LACCESS_NHNA15', 'PCT_LACCESS_NHNA15',\n",
    "#                   'LACCESS_NHPI15', 'PCT_LACCESS_NHPI15',\n",
    "#                   'LACCESS_MULTIR15', 'PCT_LACCESS_MULTIR15'\n",
    "                 ]\n",
    "                  \n",
    "columns_soc = ['PCT_NHWHITE10', 'PCT_NHBLACK10', 'PCT_HISP10', 'PCT_NHASIAN10', 'PCT_NHNA10', 'PCT_NHPI10',\n",
    "               'PCT_65OLDER10', 'PCT_18YOUNGER10', 'MEDHHINC15', 'POVRATE15','PERPOV10',\n",
    "               'CHILDPOVRATE15', 'METRO13'] \n",
    " \n",
    "columns_insec = ['FOODINSEC_15_17','VLFOODSEC_15_17']\n",
    "columns_assis = ['PCT_SNAP17','SNAP_PART_RATE16','PCT_FREE_LUNCH15','PCT_REDUCED_LUNCH15', 'PCT_WIC17']\n",
    "\n",
    "columns_stores = ['GROCPTH16','SUPERCPTH16', 'FMRKT18','FMRKTPTH18','PCT_FMRKT_FRVEG18']\n",
    "\n",
    "columns_cost = ['Total_Actual_Costs','Total_Standardized_Costs','Part_B_Drugs_Per_Capita_Actual_Costs' ]\n",
    "\n",
    "dfcolumns = columns_general + columns_cost + columns_access + columns_soc + columns_insec + columns_assis + columns_stores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all[dfcolumns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dimensions\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# descriptive analytics for numerical variables\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data\n",
    "\n",
    "The first step is erasing rows with duplicate entries, ie same exact value for two different years, 2010 and 2015.  \n",
    "Then,  delete columns with missing values and '-9999'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly for other pair of values from 2010 and 2015: erase when diference is \n",
    "df = df[df['PCH_LACCESS_POP_10_15'] !=  0]\n",
    "df = df[df['PCH_LACCESS_HHNV_10_15'] !=  0]\n",
    "df = df[df['PCH_LACCESS_SENIORS_10_15'] !=  0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that may not be as relevant\n",
    "\n",
    "delete_list = ['LACCESS_POP10','PCH_LACCESS_POP_10_15','PCT_LACCESS_POP10',\n",
    "               'LACCESS_LOWI10','PCH_LACCESS_LOWI_10_15','PCT_LACCESS_LOWI10',\n",
    "               'LACCESS_HHNV10','PCH_LACCESS_HHNV_10_15','PCT_LACCESS_HHNV10',\n",
    "               'LACCESS_CHILD10','LACCESS_CHILD_10_15','PCT_LACCESS_CHILD10', \n",
    "               'LACCESS_SENIORS10', 'PCH_LACCESS_SENIORS_10_15','PCT_LACCESS_SENIORS10']\n",
    "\n",
    "\n",
    "for column in delete_list:\n",
    "    del df[column]\n",
    "    \n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove NaNs, 0s , and -9999\n",
    "\n",
    "df = df.dropna(how='any')\n",
    "\n",
    "df = df[df[df.columns] != -9999]\n",
    "\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data\n",
    "\n",
    "for more detailed exploration check food_access notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect the data: subset at the % level \n",
    "# Boxplot:distribution of % values\n",
    "\n",
    "sb11=['PCT_OBESE_ADULTS17','PCT_DIABETES_ADULTS13']\n",
    "sb12=['PCT_LACCESS_LOWI15','PCT_LACCESS_HHNV15',\n",
    "     'PCT_LACCESS_SNAP15','PCT_LACCESS_SENIORS15',\n",
    "     'PCT_LACCESS_WHITE15', 'PCT_LACCESS_BLACK15','PCT_LACCESS_HISP15',\n",
    "     'PCT_LACCESS_NHASIAN15']\n",
    "sb13=['PCT_NHWHITE10', 'PCT_NHBLACK10', 'PCT_HISP10',\n",
    "     'PCT_NHASIAN10', 'PCT_NHNA10', 'PCT_NHPI10',\n",
    "     'PCT_65OLDER10','PCT_18YOUNGER10']\n",
    "sb14=['POVRATE15', 'CHILDPOVRATE15', 'PERPOV10',\n",
    "     'METRO13']\n",
    "sb15=['FOODINSEC_15_17','VLFOODSEC_15_17','PCT_SNAP17',\n",
    "     'SNAP_PART_RATE16','PCT_FREE_LUNCH15', 'PCT_REDUCED_LUNCH15',\n",
    "     'PCT_WIC17']\n",
    "sb16=['GROCPTH16','SUPERCPTH16', 'FMRKT18',\n",
    "      'FMRKTPTH18','PCT_FMRKT_FRVEG18']\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 12))\n",
    "\n",
    "flierprops = dict(marker='o', markerfacecolor='grey', \n",
    "                  markeredgecolor='darkcyan',markersize=2,\n",
    "                  linestyle='none')\n",
    "\n",
    "axes[0, 0].boxplot= df.boxplot(sb11,ax=axes[0,0],grid=False,\n",
    "                             rot=90, notch=True,flierprops=flierprops)\n",
    "axes[0, 0].set_title('Obesity&Diabetes')\n",
    "\n",
    "axes[0,1].boxplot = df.boxplot(sb12,grid=False, ax=axes[0,1],\n",
    "                             rot=90, notch=True,flierprops=flierprops)\n",
    "axes[0,1].set_title('access')\n",
    "\n",
    "axes[0,2].boxplot =df.boxplot(sb13,grid=False, ax=axes[0,2],\n",
    "                             rot=90, notch=True,flierprops=flierprops)\n",
    "axes[0,2].set_title('% population')\n",
    "\n",
    "axes[1,0].boxplot = df.boxplot(sb14,grid=False, ax=axes[1,0],\n",
    "                             rot=90, notch=True,flierprops=flierprops)\n",
    "axes[1,0].set_title('poverty')\n",
    "\n",
    "axes[1,1].boxplot = df.boxplot(sb15,grid=False,ax=axes[1,1], \n",
    "                             rot=90, notch=True,flierprops=flierprops)\n",
    "axes[1,1].set_title('food insecurity')\n",
    "\n",
    "axes[1,2].boxplot = df.boxplot(sb16,grid=False, ax=axes[1,2],\n",
    "                             rot=90, notch=True,flierprops=flierprops)\n",
    "axes[1,2].set_title('% food supply')\n",
    "\n",
    "fig.subplots_adjust(left=0.08, right=0.98, bottom=0.05, top=0.9,\n",
    "                    hspace=0.4, wspace=0.3)\n",
    "                             \n",
    "plt.show()                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting outlayers\n",
    "Detecting outlayers and deciding if they are the extreme cases that we are looking for or oulayers or artifacts.\n",
    "\n",
    "Note:\n",
    "Some of the data required a more detailed analysis.\n",
    "\n",
    "For example,the percent of Households with very low food security (three-years average 2015-17), 'PCT_LACCESS_POP15' have values of percent that are almost 100%, which seems an artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of values\n",
    "binsize = 1\n",
    "bins = np.arange(0, df['PCT_LACCESS_POP15'].max()+binsize, binsize)\n",
    "\n",
    "plt.figure(figsize=[8, 5])\n",
    "plt.hist(data = df, x = 'PCT_LACCESS_POP15', bins = bins, color=\"darkcyan\")\n",
    "plt.xlabel('(%) Households with very low food security (three-year average 2015-17)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariant Exploration\n",
    "\n",
    "To start I am checking correlations at the population level and the % of population.\n",
    "I am also checking similar correlations but at the population level. If they differ we may want to consider if we want to look to impact the greates number of places or the impact the \n",
    "maximun number of people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using Pearson Correlation\n",
    "# correlation matrix with percent per county for some of the variables \n",
    "\n",
    "sb=['PCT_OBESE_ADULTS17','PCT_DIABETES_ADULTS13',\n",
    "     'Total_Actual_Costs','Total_Standardized_Costs','Part_B_Drugs_Per_Capita_Actual_Costs',\n",
    "     'LACCESS_LOWI15','LACCESS_HHNV15',\n",
    "     'LACCESS_SNAP15','LACCESS_SENIORS15',\n",
    "     'LACCESS_WHITE15', 'LACCESS_BLACK15','LACCESS_HISP15',\n",
    "     'LACCESS_NHASIAN15',\n",
    "     'PCT_NHWHITE10', 'PCT_NHBLACK10', 'PCT_HISP10',\n",
    "     'PCT_NHASIAN10', 'PCT_NHNA10', 'PCT_NHPI10',     \n",
    "     'PCT_65OLDER10','PCT_18YOUNGER10','MEDHHINC15',\n",
    "     'POVRATE15', 'PERPOV10','CHILDPOVRATE15',\n",
    "     'METRO13','FOODINSEC_15_17','VLFOODSEC_15_17','PCT_SNAP17',\n",
    "     'SNAP_PART_RATE16','PCT_FREE_LUNCH15', 'PCT_REDUCED_LUNCH15',\n",
    "     'PCT_WIC17',\n",
    "     'GROCPTH16','SUPERCPTH16', 'FMRKT18','FMRKTPTH18','PCT_FMRKT_FRVEG18']\n",
    "\n",
    "corr_perc = df[sb].corr()\n",
    "\n",
    "# Set up  matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Add diverging colormap from red to blue\n",
    "cmap = sns.diverging_palette(10, 150, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr_perc,cmap=cmap,\n",
    "            square=True,\n",
    "            linewidth=.5, cbar_kws={\"shrink\": .5}, ax=ax,\n",
    "            xticklabels=corr_perc.columns.values,\n",
    "            yticklabels=corr_perc.columns.values)\n",
    "\n",
    "fig.subplots_adjust(left=0.08, right=0.98, bottom=0.05, top=0.9,\n",
    "                    hspace=0.4, wspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations with the target variables\n",
    "\n",
    "Check what are the variables that seem to have highest correlation with 'Diabesity'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation with output variable: Adult Diabetes Rate (2013)\n",
    "corr_target = abs(corr_perc['PCT_DIABETES_ADULTS13'])\n",
    "\n",
    "# Selecting highly correlated features\n",
    "relevant_features = corr_target[corr_target > 0.4]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation with output variable: Adult obesity rate, 2017\n",
    "cor_target = abs(corr_perc['PCT_OBESE_ADULTS17'])\n",
    "\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.5]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these correlations we can see that Adult diabetes rate PCT_DIABETES_ADULTS13 and household very low food security (%, three-year average 2015-17),VLFOODSEC_15_17 are highly correlated with adult obesity rate. \n",
    "\n",
    "In addition poverty rate, POVRATE15, child poverty rate, CHILDPOVRATE15, percent of students eligible for free lunch , PCT_FREE_LUNCH15 and median household income, MEDHHINC15, and obesity are correlated with the adult diabetes rate.\n",
    "\n",
    "However this regression analysis assumes that the independent variables are uncorrelated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df[[\"PCT_DIABETES_ADULTS13\",\"VLFOODSEC_15_17\"]].corr())\n",
    "print(df[[\"CHILDPOVRATE15\",\"POVRATE15\"]].corr())\n",
    "print(df[[\"CHILDPOVRATE15\",\"PCT_FREE_LUNCH15\"]].corr())\n",
    "print(df[[\"CHILDPOVRATE15\",\"MEDHHINC15\"]].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df[[\"PCT_FREE_LUNCH15\",\"MEDHHINC15\"]].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the correlation coeficients household income, poverty rate, child poverty rate, and students elegible for free lunch are variables that are highly correlated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# There is a positive trend between elegibility for free lunches and percent\n",
    "# of african american population, but negative correlation with \n",
    "# reduced lunches. \n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "\n",
    "ax1 = plt.subplot2grid((3,3), (0,0)) \n",
    "ax2 = plt.subplot2grid((3,3), (0,1))   \n",
    "ax3 = plt.subplot2grid((3,3), (0,2))\n",
    "ax4 = plt.subplot2grid((3,3), (1,0))  # \n",
    "ax5 = plt.subplot2grid((3,3), (1,1), colspan=2)  \n",
    "ax6 = plt.subplot2grid((3,3), (2,0))  # \n",
    "ax7 = plt.subplot2grid((3,3), (2,1), colspan=2)  \n",
    "\n",
    "data = df[['PCT_NHBLACK10','PCT_FREE_LUNCH15','PCT_REDUCED_LUNCH15']]\n",
    "data = data.loc[(data[\"PCT_REDUCED_LUNCH15\"]!=0)]\n",
    "PCT_NHBLACK10 = df.iloc[:,29]\n",
    "PCT_FREE_LUNCH15 = df.iloc[:,45]\n",
    "PCT_REDUCED_LUNCH15 = df.iloc[:,46]\n",
    "\n",
    "ax1.hist(data['PCT_NHBLACK10'], bins=100,color='darkcyan')\n",
    "ax1.set(title='% African-American', xlabel='% African-American', ylabel='frequency')\n",
    "\n",
    "ax2.hist(data['PCT_FREE_LUNCH15'], bins=100,color='darkcyan')\n",
    "ax2.set(title='% Free Lunch', xlabel='% free lunch', ylabel='frequency')\n",
    "\n",
    "ax3.hist(data['PCT_REDUCED_LUNCH15'], bins=100,color='darkcyan')\n",
    "ax3.set(title='% Reduced Lunch', xlabel='% reduced lunch', ylabel='frequency')\n",
    "\n",
    "ax4.scatter(PCT_NHBLACK10, PCT_FREE_LUNCH15, color='darkcyan', alpha=0.3)\n",
    "ax4.set(title='all data', xlabel='% African-American', ylabel='% Students Free Lunch')\n",
    "sns.regplot(data['PCT_NHBLACK10'], data['PCT_FREE_LUNCH15'],ax=ax5, color='darkcyan')\n",
    "ax5.set(title='% African-American population (2010) vs. % Students eligible for free lunch (2015)', xlabel='% African-American', ylabel='% Students Free Lunch')\n",
    "\n",
    "ax6.scatter(PCT_NHBLACK10, PCT_REDUCED_LUNCH15, color='darkcyan', alpha=0.3)\n",
    "ax6.set(title='all data', xlabel='% African-American', ylabel='% Students Reduced Lunch')\n",
    "sns.regplot(data['PCT_NHBLACK10'], data['PCT_REDUCED_LUNCH15'],ax=ax7, color='darkcyan')\n",
    "ax7.set(title='% African-American population (2010) vs. % Students eligible for reduced-price lunch (2015)', xlabel='% African-American', ylabel='% Students Reduced Lunch')\n",
    "\n",
    "fig.subplots_adjust(left=0.08, right=0.98, bottom=0.05, top=0.9,\n",
    "                    hspace=0.4, wspace=0.3)\n",
    "fig.tight_layout()\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The higher the percentage of white population the lower the free school lunches. \n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "\n",
    "ax1 = plt.subplot2grid((3,3), (0,0)) \n",
    "ax2 = plt.subplot2grid((3,3), (0,1))   \n",
    "\n",
    "ax4 = plt.subplot2grid((3,3), (1,0))  # \n",
    "ax5 = plt.subplot2grid((3,3), (1,1), colspan=2)  \n",
    "ax6 = plt.subplot2grid((3,3), (2,0))  # \n",
    "ax7 = plt.subplot2grid((3,3), (2,1), colspan=2)  \n",
    "\n",
    "data = df[['PCT_NHWHITE10','PCT_FREE_LUNCH15','PCT_REDUCED_LUNCH15']]\n",
    "data = data.loc[(data[\"PCT_REDUCED_LUNCH15\"]!=0)]\n",
    "PCT_NHWHITE10 = df.iloc[:,28]\n",
    "PCT_FREE_LUNCH15 = df.iloc[:,45]\n",
    "PCT_REDUCED_LUNCH15 = df.iloc[:,46]\n",
    "\n",
    "ax1.hist(data['PCT_NHWHITE10'], bins=100,color='indianred')\n",
    "ax1.set(title='% white population', xlabel='% white', ylabel='frequency')\n",
    "\n",
    "ax2.hist(df['PCT_NHBLACK10'], bins=100,color='darkcyan')\n",
    "ax2.set(title='% African-American', xlabel='% African-American', ylabel='frequency')\n",
    "\n",
    "ax4.scatter(PCT_NHWHITE10, PCT_FREE_LUNCH15, color='indianred', alpha=0.3)\n",
    "ax4.set(title='all data', xlabel='% white', ylabel='% Students Free Lunch')\n",
    "sns.regplot(data['PCT_NHWHITE10'], data['PCT_FREE_LUNCH15'],ax=ax5, color='indianred')\n",
    "ax5.set(title='% white population (2010) vs. % Students eligible for free lunch (2015)', xlabel='% white', ylabel='% Students Free Lunch')\n",
    "\n",
    "ax6.scatter(PCT_NHWHITE10, PCT_REDUCED_LUNCH15, color='indianred', alpha=0.3)\n",
    "ax6.set(title='all data', xlabel='% white', ylabel='% Students Reduced Lunch')\n",
    "sns.regplot(data['PCT_NHWHITE10'], data['PCT_REDUCED_LUNCH15'],ax=ax7, color='indianred')\n",
    "ax7.set(title='% white population (2010) vs. % Students eligible for reduced-price lunch (2015)', xlabel='% white', ylabel='% Students Reduced Lunch')\n",
    "\n",
    "fig.subplots_adjust(left=0.08, right=0.98, bottom=0.05, top=0.9,\n",
    "                    hspace=0.4, wspace=0.3) \n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model to predict location\n",
    "\n",
    "To look for a more specific locations one posible approach is using \n",
    "few weigthed parameters and set up a system of scores based on the \n",
    "quantile distributions of different relevant parameters.\n",
    "\n",
    "For example, if diabetes and obesity have an association with model child poverty rate, and elegibility to school free lunch those features can be weigthed and fed back to each county to calculate an score base on the deviation to the mean.\n",
    "\n",
    "The counties with the higher score could be proposed as a condidates for the intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature selection Using Pearson Correlation\n",
    "\n",
    "df1 = df.copy()\n",
    "cor = df1.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation with output variable\n",
    "cor_target = abs(cor[\"PCT_DIABETES_ADULTS13\"])\n",
    "\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.4]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bining the variables of interest \n",
    "\n",
    "columns_list = [\n",
    "    'PCT_OBESE_ADULTS17',\n",
    "    'PCT_DIABETES_ADULTS13',\n",
    "    'CHILDPOVRATE15',\n",
    "    'PCT_FREE_LUNCH15',\n",
    "    'PCT_NHBLACK10'\n",
    "]\n",
    "\n",
    "count = 1\n",
    "\n",
    "for col in columns_list:\n",
    "    quartiles = df1[col].quantile([1/4,1/2,3/4]) # this are quartiles\n",
    "    lowerq = quartiles[0.25]\n",
    "    upperq = quartiles[0.75]\n",
    "    iqr = upperq-lowerq #interquartile range\n",
    "    lower_bound = lowerq - (1.5*iqr)\n",
    "    upper_bound = upperq + (1.5*iqr)\n",
    "    col_bins = [df1[col].min()-1,(quartiles[0.5]), (quartiles[0.75]),(upperq + (1.5*iqr)),df1[col].max()+2]\n",
    "    col_labels=[0,1,4,6]\n",
    "    name = 'col_' + str(count)\n",
    "    df1[name]= pd.cut(df1[col], bins=col_bins,labels=False)\n",
    "    count +=1\n",
    "\n",
    "# add up the scores\n",
    "\n",
    "df1['sum'] = df1['col_1'] + df1['col_2'] + df1['col_3'] + df1['col_4'] + df1['col_5']\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a risk score \n",
    "\n",
    "risk_bins = [-1,6,10,12,15]\n",
    "group_names = ['lowest (<6)', 'low (6-10)', 'high (10-12)', 'highest (12-15)']\n",
    "\n",
    "df1['risk score'] = pd.cut(df1['sum'], risk_bins, labels= group_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "# Dataframe grouped by risk scores\n",
    "\n",
    "risk_score = df1.groupby('risk score')\n",
    "means_risk = risk_score.mean()\n",
    "errors_risk = risk_score.std()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "ax1 = plt.subplot2grid((2,3), (0,0)) \n",
    "ax2 = plt.subplot2grid((2,3), (0,1)) \n",
    "\n",
    "ax4 = plt.subplot2grid((2,3), (1,0)) \n",
    "ax5 = plt.subplot2grid((2,3), (1,1)) \n",
    "ax6 = plt.subplot2grid((2,3), (1,2)) \n",
    "\n",
    "means_risk.plot.bar(yerr=errors_risk, y='PCT_LACCESS_LOWI15', ax=ax1, capsize=4, rot=0, color=['darkcyan']);\n",
    "ax1.set(title='% Low income & low access to store, 2015', xlabel='risk score', ylabel='% pop')\n",
    "means_risk.plot.bar(yerr=errors_risk, y='PCT_LACCESS_HHNV15', ax=ax2, capsize=4, rot=0);\n",
    "ax2.set(title='% Households, no car & low access to store, 2015', xlabel='risk score', ylabel='% pop')\n",
    "\n",
    "means_risk.plot.bar(yerr=errors_risk, y='PCT_LACCESS_SNAP15', ax=ax4, capsize=4, rot=0,color='darkcyan');\n",
    "ax4.set(title='% SNAP households, low access to store, 2015', xlabel='risk score', ylabel='% pop')\n",
    "means_risk.plot.bar(yerr=errors_risk, y='PCT_SNAP17', ax=ax5, capsize=4, rot=0);\n",
    "ax5.set(title='SNAP participants (% pop), 2017', xlabel='risk score', ylabel='% pop')\n",
    "means_risk.plot.bar(yerr=errors_risk, y='PCT_WIC17', ax=ax6, capsize=4, rot=0,color='darkcyan');\n",
    "ax6.set(title='WIC participants (% pop), 2017', xlabel='risk score', ylabel='% pop')\n",
    "\n",
    "fig.subplots_adjust(left=0.08, right=0.98, bottom=0.05, top=0.9,\n",
    "                    hspace=0.4, wspace=0.3) \n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographical locations\n",
    "\n",
    "The counties with the higher score could be proposed as a condidates for the intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_risk1 = df1[['FIPS','State','County']].loc[df1['risk score'] == 'highest (12-15)']\n",
    "\n",
    "states_risk1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple regression: model and predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection:\n",
    "\n",
    "There are basically three appraches: \n",
    "- 1- filter method\n",
    "- 2- wrapper method\n",
    "- 3- embedded method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Filter \n",
    "\n",
    "Filtering is done using correlation matrix, usually Pearson correlation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pearson Correlation\n",
    "\n",
    "\n",
    "df_subset= df[sb].copy()\n",
    "\n",
    "cor = df_subset.corr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Correlation with output variable\n",
    "cor_target = abs(cor[\"PCT_DIABETES_ADULTS13\"])\n",
    "\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.5]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assumes that they are independent. However, in these case they are not independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One quick to implement feature selections and reduce the features is by using is using VIF, variable inflation factors. \n",
    "VIF determines the strenght of the correlation between variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vif(Z):\n",
    "\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = Z.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(Z.values, i) for i in range(Z.shape[1])]\n",
    "\n",
    "    return(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_var = df_subset.iloc[:,:-1]\n",
    "calc_vif(df_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those variables with a coeficient closer to 1 are not correlated with the other variables. The higher the number the higher the collinearity of the particular variable and the rest.\n",
    "This step will help to eliminate variables and also consider if two variables could be convined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining variables\n",
    "# df2 = df_subset.copy()\n",
    "df_subset['poverty_rate'] = df_subset.apply(lambda x: x['CHILDPOVRATE15'] + x['POVRATE15'],axis=1)\n",
    "df_subset = df_subset.drop(['CHILDPOVRATE15','POVRATE15',],axis=1)\n",
    "calc_vif(df_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next step will be combining several of them and reducing the correlation between variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2- Wrapper method\n",
    "\n",
    "Requires a machine learning algorith and uses performance to evaluate the features.\n",
    "\n",
    "There are several methods like Backward Elimination, Forward Selection, Bidirectional Elimination and Recursive Feature EliminationRFE. \n",
    "\n",
    "##### Backward Elimination\n",
    "\n",
    "It checks the performance of the model with all the features and then iterates removing the worst performing one until the performance of the model is acceptable. \n",
    "The performance metric used here os the pvalue. If the pvalue is greater than 0.o5 then the veature is removed.\n",
    "\n",
    "I am using the Ordinary Least Squares (OLS). Simple linear regression model: estimation of the parameters based on minimizing sum of squares of residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non numerical variables\n",
    "df2 = df.drop(columns = [\"State\",\"County\"], axis = 1)\n",
    "# df1.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Feature Matrix\n",
    "X = df2.drop(\"PCT_DIABETES_ADULTS13\",1) \n",
    "\n",
    "#Target Variable\n",
    "y = df2[\"PCT_DIABETES_ADULTS13\"]          \n",
    "\n",
    "# y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Adding constant column of ones, required for sm.OLS model\n",
    "X_1 = sm.add_constant(X)\n",
    "\n",
    "# Fitting sm.OLS model\n",
    "model = sm.OLS(y,X_1.astype(float)).fit()\n",
    "model.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Backward Elimination: we are going to eliminate features that has p > 0.05\n",
    "\n",
    "cols = list(X.columns)\n",
    "pmax = 1\n",
    "while (len(cols)>0):\n",
    "    p= []\n",
    "    X_1 = X[cols]\n",
    "    X_1 = sm.add_constant(X_1)\n",
    "    model = sm.OLS(y,X_1).fit()\n",
    "    p = pd.Series(model.pvalues.values[1:],index = cols)      \n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax<0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "selected_features_BE = cols\n",
    "no_features = len(selected_features_BE)\n",
    "\n",
    "print(\"Optimum number of features: %d\" %no_features)\n",
    "\n",
    "print(selected_features_BE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is not eliminating any features.\n",
    "\n",
    "\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RFE (Recursive Feature Elimination)\n",
    "\n",
    "RFE method removes atrributes/features recursively and builds the model with the rest of the features. \n",
    "As metrics, this model ranks the features by accuracy. \n",
    "ranks all features with 1 most important, and labels as True if it is relevant and False if it is not relevant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "#Initializing RFE model\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "\n",
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X,y) \n",
    "\n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# there are 50 variables (50 columns)\n",
    "\n",
    "#no of features\n",
    "nof_list = np.arange(1,50)            \n",
    "high_score = 0\n",
    "\n",
    "#Variable to store the optimum features\n",
    "no_features = 0           \n",
    "score_list =[]\n",
    "for n in range(len(nof_list)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model,n_features_to_select = nof_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train,y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    score = model.score(X_test_rfe,y_test)\n",
    "    score_list.append(score)\n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        no_features = nof_list[n]\n",
    "        \n",
    "print(\"Optimum number of features: %d\" %no_features)\n",
    "\n",
    "print(\"Score with %d features: %f\" % (no_features, high_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = list(X.columns)\n",
    "model = LinearRegression()\n",
    "\n",
    "#Initializing RFE model\n",
    "rfe = RFE(model, n_features_to_select=49)     \n",
    "\n",
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X,y)  \n",
    "\n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y)              \n",
    "temp = pd.Series(rfe.support_,index = cols)\n",
    "selected_features_rfe = temp[temp==True].index\n",
    "\n",
    "print(selected_features_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Embedded Method\n",
    "\n",
    "Requires a machine learning algorith. These methods are iterative and check for the features that contribute the most to the training for each iteration. These methods use regularization  (reduce variance at the cost of introducing some bias). \n",
    "\n",
    "There are three popular regularization techniques: \n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Elastic Net\n",
    "\n",
    "\n",
    "#### Lasso Regression \n",
    "\n",
    "Least Absolute Shrinkage and Selection Operator, Lasso, regularization penalizes the coeficient of a feature and makes it 0 if the feature is irrelevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg = LassoCV()\n",
    "reg.fit(X, y)\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
    "print(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\n",
    "coef = pd.Series(reg.coef_, index = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_coef = coef.sort_values()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Feature importance using Lasso Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step\n",
    "\n",
    "estimate the cost by correlating a decrease on the number of {choose a variable like hospitalizations} with an expected decrease in {variable like total cost}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
